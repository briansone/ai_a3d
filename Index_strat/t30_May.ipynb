{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from Functions.conn import db_upload\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_data shape is \n",
      "(4598, 3)\n",
      "trainingData groupData shape expecting 752 x 2304 (4310, 576)\n",
      "training label row 1 0.8127757961653643\n",
      "training label shape expecting 753 * 1  (4310,)\n",
      "input layer set to: 576\n",
      "4310/4310 [==============================] - 0s 70us/sample - loss: 0.0090 - mae: 0.0603\n",
      "4310/4310 [==============================] - 0s 41us/sample - loss: 0.0019 - mae: 0.0340\n",
      "mae: 3.40%\n",
      "prediction is: \n",
      "trainingData groupData shape expecting unsure (4598, 2)\n",
      "min in trainingData is:  14050.6\n",
      "max in trainingData is:  77845.45\n",
      "trainingData groupData shape expecting unsure (4598, 2)\n",
      "input data is:  [0.01202918 0.0613838  0.01394313 0.05736234 0.00994594 0.04970966\n",
      " 0.00679052 0.04516501 0.00559606 0.05429747 0.00855398 0.05644029\n",
      " 0.01078614 0.05524888 0.01153228 0.05805993 0.01531158 0.06348173\n",
      " 0.01912537 0.06290156 0.02236074 0.0660027  0.02135282 0.07113787\n",
      " 0.02172119 0.07049554 0.02554752 0.07097902 0.0225269  0.06880857\n",
      " 0.02355363 0.06744276 0.02231528 0.07333767 0.02717461 0.07734532\n",
      " 0.02918104 0.07594325 0.0271699  0.0755392  0.0280775  0.07737467\n",
      " 0.03017642 0.07242771 0.02722007 0.07037467 0.02534844 0.07155746\n",
      " 0.0257074  0.07554438 0.02827971 0.07430117 0.03081597 0.07440649\n",
      " 0.03153546 0.07802218 0.03323623 0.07912036 0.03168438 0.07275405\n",
      " 0.0314226  0.06827329 0.0311812  0.06829747 0.03135833 0.07423382\n",
      " 0.03068743 0.07499184 0.0295933  0.07879746 0.03418771 0.08268942\n",
      " 0.0316734  0.08203846 0.03297288 0.08271705 0.03426766 0.08315217\n",
      " 0.03597156 0.08267043 0.03491034 0.08775553 0.03840279 0.08833742\n",
      " 0.03901412 0.08858779 0.03729455 0.08773826 0.03847176 0.09075824\n",
      " 0.04011923 0.08872765 0.03881818 0.08736702 0.03878369 0.09060629\n",
      " 0.04011923 0.09232953 0.04072586 0.090406   0.04052208 0.08535543\n",
      " 0.0386771  0.08822519 0.03931195 0.08804043 0.03864262 0.08571976\n",
      " 0.0383009  0.08934236 0.04011766 0.09565168 0.04050327 0.09487121\n",
      " 0.04198458 0.09273357 0.04112244 0.08826663 0.04103936 0.0874948\n",
      " 0.03812533 0.08761394 0.03737763 0.08331966 0.03571291 0.08422272\n",
      " 0.03561886 0.08697334 0.03802345 0.08906091 0.03830403 0.0906909\n",
      " 0.03985118 0.08838577 0.0425003  0.08959791 0.04371356 0.0895392\n",
      " 0.0458595  0.09266278 0.04659937 0.09361937 0.0462263  0.09046816\n",
      " 0.04804306 0.09211369 0.04601625 0.09469855 0.04726871 0.0964615\n",
      " 0.04814182 0.09460703 0.04627646 0.09366253 0.04545978 0.09231917\n",
      " 0.04519801 0.09266796 0.04586107 0.09001404 0.04505693 0.08826145\n",
      " 0.04497385 0.09015735 0.04649435 0.08564379 0.04395339 0.08840304\n",
      " 0.04347843 0.08565069 0.04616047 0.08363565 0.04620592 0.08579919\n",
      " 0.04688623 0.08676786 0.04779226 0.08704241 0.04861991 0.07911518\n",
      " 0.04710725 0.08230264 0.04949616 0.08638799 0.04803836 0.08443856\n",
      " 0.04970307 0.07767512 0.04856818 0.0766063  0.04646457 0.0750471\n",
      " 0.04568864 0.07428563 0.04852743 0.07820176 0.05064986 0.08204709\n",
      " 0.05101039 0.08096619 0.05051035 0.0837427  0.05147594 0.08326959\n",
      " 0.05095709 0.08028069 0.05057932 0.07520422 0.04802739 0.07879401\n",
      " 0.04777188 0.07464478 0.04681569 0.07681523 0.04690818 0.0815947\n",
      " 0.0478722  0.08160506 0.04621376 0.08183989 0.0463423  0.08107669\n",
      " 0.04661191 0.08719435 0.04752735 0.0925868  0.04847413 0.09070817\n",
      " 0.04991469 0.09482287 0.05356075 0.09355548 0.05268764 0.09520965\n",
      " 0.05203241 0.09617659 0.05089282 0.09552736 0.04993977 0.09279573\n",
      " 0.05009182 0.09254364 0.04663229 0.09574837 0.05058089 0.09560506\n",
      " 0.05118438 0.09676367 0.05174085 0.09227255 0.0493253  0.09270422\n",
      " 0.04942562 0.09151108 0.04873277 0.08739983 0.0456181  0.08905745\n",
      " 0.04779853 0.09054931 0.04807441 0.09123653 0.04592377 0.08863959\n",
      " 0.0451886  0.08591315 0.0448265  0.08614798 0.04620122 0.08527082\n",
      " 0.04619025 0.0859615  0.04418068 0.08414502 0.04528265 0.08725306\n",
      " 0.04450202 0.08071927 0.04349724 0.08250985 0.04456002 0.080443\n",
      " 0.04602252 0.08675578 0.04718719 0.08621014 0.04766999 0.08582336\n",
      " 0.04777188 0.08599431 0.04826252 0.08175528 0.04749443 0.08007521\n",
      " 0.04510866 0.07816722 0.04556167 0.07408188 0.04349411 0.07143486\n",
      " 0.04327309 0.07424073 0.04408977 0.07659766 0.04424025 0.07616772\n",
      " 0.04453338 0.07345336 0.04295331 0.06837862 0.04134817 0.06760506\n",
      " 0.04251754 0.07726762 0.04241095 0.07344473 0.04180902 0.06903822\n",
      " 0.03957373 0.07386086 0.0405017  0.07942771 0.0453673  0.07799283\n",
      " 0.04688153 0.07975405 0.04847413 0.08186752 0.04695834 0.07826564\n",
      " 0.04569334 0.075332   0.0467091  0.07685149 0.04543    0.07956239\n",
      " 0.04649905 0.07631621 0.04641754 0.07404907 0.04605074 0.07239835\n",
      " 0.04651629 0.07169386 0.04550524 0.07230166 0.0459128  0.06582485\n",
      " 0.0436681  0.0658283  0.04391891 0.0681766  0.04369318 0.06310359\n",
      " 0.04404744 0.06270817 0.04291569 0.05929969 0.04294077 0.06097285\n",
      " 0.04180745 0.05835692 0.04027284 0.0577992  0.04070862 0.06488898\n",
      " 0.04168362 0.06513245 0.03915363 0.06094177 0.03913169 0.05758681\n",
      " 0.03696223 0.05338405 0.03593394 0.05426294 0.03530222 0.05020004\n",
      " 0.03150881 0.04968376 0.0341595  0.05388479 0.03440246 0.05368277\n",
      " 0.03534611 0.04849234 0.03409836 0.04349876 0.03202453 0.04495608\n",
      " 0.03431468 0.05355499 0.0358054  0.05066106 0.03776167 0.04475406\n",
      " 0.03521287 0.03682338 0.03254024 0.03852934 0.03289607 0.03706166\n",
      " 0.03213112 0.03621213 0.0297861  0.03207153 0.02698023 0.03331474\n",
      " 0.02936287 0.02731104 0.02595194 0.0190143  0.02438598 0.0121628\n",
      " 0.02432485 0.00672545 0.01868333 0.01795239 0.02068192 0.01679033\n",
      " 0.01648722 0.02026615 0.02010507 0.03160877 0.02395491 0.03256191\n",
      " 0.02415085 0.03475825 0.02344703 0.02817612 0.02159265 0.02316354\n",
      " 0.01805005 0.01589072 0.01622388 0.02202392 0.02139985 0.02628712\n",
      " 0.02347368 0.03333892 0.02582497 0.03417118 0.02576384 0.0329884\n",
      " 0.02629209 0.02820029 0.02528104 0.03695115 0.02780475 0.03958953\n",
      " 0.02822955 0.0392511  0.02947573 0.04465046 0.0317361  0.04138183\n",
      " 0.03085516 0.04433793 0.03335379 0.04762382 0.03247127 0.04226072\n",
      " 0.03230825 0.04400295 0.03102131 0.04077749 0.02923433 0.03664725\n",
      " 0.02876721 0.03664725 0.02797091 0.03622939 0.03023128 0.02687419\n",
      " 0.02633598 0.03073334 0.0279568  0.02723507 0.02612593 0.03088011\n",
      " 0.02867473 0.03310927 0.0279474  0.03473926 0.03097429 0.03474271\n",
      " 0.03053695 0.02924666 0.02962778 0.02996324 0.02963249 0.03028786\n",
      " 0.03211545 0.02596077 0.02920769 0.02497829 0.02687991 0.0185481\n",
      " 0.02552714 0.0190592  0.02342979 0.01618253 0.02245636 0.01263591\n",
      " 0.01907991 0.01770202 0.0189357  0.02150764 0.02055182 0.0147183\n",
      " 0.01767698 0.01174149 0.01898586 0.01977923 0.01987778 0.01489787\n",
      " 0.01996713 0.01269635 0.01936363 0.00817069 0.01522693 0.0043996\n",
      " 0.0157113  0.00774593 0.01313899 0.00239492 0.01184735 0.00911001\n",
      " 0.01409832 0.01685076 0.0169857  0.01835989 0.01981038 0.02817957\n",
      " 0.01972416 0.02295979 0.01939655 0.02769264 0.02108634 0.02897385\n",
      " 0.01946239 0.03275357 0.02050479 0.03039664 0.02282473 0.03187296\n",
      " 0.0221601  0.02851282 0.02103932 0.03224938 0.02320563 0.03041736\n",
      " 0.02126034 0.02844721 0.02084965 0.03059866 0.02410539 0.0294176\n",
      " 0.02148763 0.03316797 0.02505688 0.03499136 0.02469165 0.03673704\n",
      " 0.02671846 0.03885051 0.02378248 0.03368943 0.02137633 0.03175382\n",
      " 0.01871311 0.02717464 0.01948903 0.02885298 0.01859554 0.02882362]\n",
      "input data shape is:  (576,)\n",
      "1/1 [==============================] - 0s 13ms/sample\n",
      "prediction scaled is: [[0.06849593]]\n",
      "prediction is: [[18420.287]]\n",
      "1 record inserted.\n",
      "index_data shape is \n",
      "(4598, 3)\n",
      "trainingData groupData shape expecting 752 x 2304 (4304, 576)\n",
      "training label row 1 0.7890790557545004\n",
      "training label shape expecting 753 * 1  (4304,)\n",
      "input layer set to: 576\n",
      "4304/4304 [==============================] - 0s 80us/sample - loss: 0.0089 - mae: 0.0602\n",
      "4304/4304 [==============================] - 0s 36us/sample - loss: 0.0024 - mae: 0.0407\n",
      "mae: 4.07%\n",
      "prediction is: \n",
      "trainingData groupData shape expecting unsure (4598, 2)\n",
      "min in trainingData is:  14050.6\n",
      "max in trainingData is:  77845.45\n",
      "trainingData groupData shape expecting unsure (4598, 2)\n",
      "input data is:  [0.01078614 0.05524888 0.01153228 0.05805993 0.01531158 0.06348173\n",
      " 0.01912537 0.06290156 0.02236074 0.0660027  0.02135282 0.07113787\n",
      " 0.02172119 0.07049554 0.02554752 0.07097902 0.0225269  0.06880857\n",
      " 0.02355363 0.06744276 0.02231528 0.07333767 0.02717461 0.07734532\n",
      " 0.02918104 0.07594325 0.0271699  0.0755392  0.0280775  0.07737467\n",
      " 0.03017642 0.07242771 0.02722007 0.07037467 0.02534844 0.07155746\n",
      " 0.0257074  0.07554438 0.02827971 0.07430117 0.03081597 0.07440649\n",
      " 0.03153546 0.07802218 0.03323623 0.07912036 0.03168438 0.07275405\n",
      " 0.0314226  0.06827329 0.0311812  0.06829747 0.03135833 0.07423382\n",
      " 0.03068743 0.07499184 0.0295933  0.07879746 0.03418771 0.08268942\n",
      " 0.0316734  0.08203846 0.03297288 0.08271705 0.03426766 0.08315217\n",
      " 0.03597156 0.08267043 0.03491034 0.08775553 0.03840279 0.08833742\n",
      " 0.03901412 0.08858779 0.03729455 0.08773826 0.03847176 0.09075824\n",
      " 0.04011923 0.08872765 0.03881818 0.08736702 0.03878369 0.09060629\n",
      " 0.04011923 0.09232953 0.04072586 0.090406   0.04052208 0.08535543\n",
      " 0.0386771  0.08822519 0.03931195 0.08804043 0.03864262 0.08571976\n",
      " 0.0383009  0.08934236 0.04011766 0.09565168 0.04050327 0.09487121\n",
      " 0.04198458 0.09273357 0.04112244 0.08826663 0.04103936 0.0874948\n",
      " 0.03812533 0.08761394 0.03737763 0.08331966 0.03571291 0.08422272\n",
      " 0.03561886 0.08697334 0.03802345 0.08906091 0.03830403 0.0906909\n",
      " 0.03985118 0.08838577 0.0425003  0.08959791 0.04371356 0.0895392\n",
      " 0.0458595  0.09266278 0.04659937 0.09361937 0.0462263  0.09046816\n",
      " 0.04804306 0.09211369 0.04601625 0.09469855 0.04726871 0.0964615\n",
      " 0.04814182 0.09460703 0.04627646 0.09366253 0.04545978 0.09231917\n",
      " 0.04519801 0.09266796 0.04586107 0.09001404 0.04505693 0.08826145\n",
      " 0.04497385 0.09015735 0.04649435 0.08564379 0.04395339 0.08840304\n",
      " 0.04347843 0.08565069 0.04616047 0.08363565 0.04620592 0.08579919\n",
      " 0.04688623 0.08676786 0.04779226 0.08704241 0.04861991 0.07911518\n",
      " 0.04710725 0.08230264 0.04949616 0.08638799 0.04803836 0.08443856\n",
      " 0.04970307 0.07767512 0.04856818 0.0766063  0.04646457 0.0750471\n",
      " 0.04568864 0.07428563 0.04852743 0.07820176 0.05064986 0.08204709\n",
      " 0.05101039 0.08096619 0.05051035 0.0837427  0.05147594 0.08326959\n",
      " 0.05095709 0.08028069 0.05057932 0.07520422 0.04802739 0.07879401\n",
      " 0.04777188 0.07464478 0.04681569 0.07681523 0.04690818 0.0815947\n",
      " 0.0478722  0.08160506 0.04621376 0.08183989 0.0463423  0.08107669\n",
      " 0.04661191 0.08719435 0.04752735 0.0925868  0.04847413 0.09070817\n",
      " 0.04991469 0.09482287 0.05356075 0.09355548 0.05268764 0.09520965\n",
      " 0.05203241 0.09617659 0.05089282 0.09552736 0.04993977 0.09279573\n",
      " 0.05009182 0.09254364 0.04663229 0.09574837 0.05058089 0.09560506\n",
      " 0.05118438 0.09676367 0.05174085 0.09227255 0.0493253  0.09270422\n",
      " 0.04942562 0.09151108 0.04873277 0.08739983 0.0456181  0.08905745\n",
      " 0.04779853 0.09054931 0.04807441 0.09123653 0.04592377 0.08863959\n",
      " 0.0451886  0.08591315 0.0448265  0.08614798 0.04620122 0.08527082\n",
      " 0.04619025 0.0859615  0.04418068 0.08414502 0.04528265 0.08725306\n",
      " 0.04450202 0.08071927 0.04349724 0.08250985 0.04456002 0.080443\n",
      " 0.04602252 0.08675578 0.04718719 0.08621014 0.04766999 0.08582336\n",
      " 0.04777188 0.08599431 0.04826252 0.08175528 0.04749443 0.08007521\n",
      " 0.04510866 0.07816722 0.04556167 0.07408188 0.04349411 0.07143486\n",
      " 0.04327309 0.07424073 0.04408977 0.07659766 0.04424025 0.07616772\n",
      " 0.04453338 0.07345336 0.04295331 0.06837862 0.04134817 0.06760506\n",
      " 0.04251754 0.07726762 0.04241095 0.07344473 0.04180902 0.06903822\n",
      " 0.03957373 0.07386086 0.0405017  0.07942771 0.0453673  0.07799283\n",
      " 0.04688153 0.07975405 0.04847413 0.08186752 0.04695834 0.07826564\n",
      " 0.04569334 0.075332   0.0467091  0.07685149 0.04543    0.07956239\n",
      " 0.04649905 0.07631621 0.04641754 0.07404907 0.04605074 0.07239835\n",
      " 0.04651629 0.07169386 0.04550524 0.07230166 0.0459128  0.06582485\n",
      " 0.0436681  0.0658283  0.04391891 0.0681766  0.04369318 0.06310359\n",
      " 0.04404744 0.06270817 0.04291569 0.05929969 0.04294077 0.06097285\n",
      " 0.04180745 0.05835692 0.04027284 0.0577992  0.04070862 0.06488898\n",
      " 0.04168362 0.06513245 0.03915363 0.06094177 0.03913169 0.05758681\n",
      " 0.03696223 0.05338405 0.03593394 0.05426294 0.03530222 0.05020004\n",
      " 0.03150881 0.04968376 0.0341595  0.05388479 0.03440246 0.05368277\n",
      " 0.03534611 0.04849234 0.03409836 0.04349876 0.03202453 0.04495608\n",
      " 0.03431468 0.05355499 0.0358054  0.05066106 0.03776167 0.04475406\n",
      " 0.03521287 0.03682338 0.03254024 0.03852934 0.03289607 0.03706166\n",
      " 0.03213112 0.03621213 0.0297861  0.03207153 0.02698023 0.03331474\n",
      " 0.02936287 0.02731104 0.02595194 0.0190143  0.02438598 0.0121628\n",
      " 0.02432485 0.00672545 0.01868333 0.01795239 0.02068192 0.01679033\n",
      " 0.01648722 0.02026615 0.02010507 0.03160877 0.02395491 0.03256191\n",
      " 0.02415085 0.03475825 0.02344703 0.02817612 0.02159265 0.02316354\n",
      " 0.01805005 0.01589072 0.01622388 0.02202392 0.02139985 0.02628712\n",
      " 0.02347368 0.03333892 0.02582497 0.03417118 0.02576384 0.0329884\n",
      " 0.02629209 0.02820029 0.02528104 0.03695115 0.02780475 0.03958953\n",
      " 0.02822955 0.0392511  0.02947573 0.04465046 0.0317361  0.04138183\n",
      " 0.03085516 0.04433793 0.03335379 0.04762382 0.03247127 0.04226072\n",
      " 0.03230825 0.04400295 0.03102131 0.04077749 0.02923433 0.03664725\n",
      " 0.02876721 0.03664725 0.02797091 0.03622939 0.03023128 0.02687419\n",
      " 0.02633598 0.03073334 0.0279568  0.02723507 0.02612593 0.03088011\n",
      " 0.02867473 0.03310927 0.0279474  0.03473926 0.03097429 0.03474271\n",
      " 0.03053695 0.02924666 0.02962778 0.02996324 0.02963249 0.03028786\n",
      " 0.03211545 0.02596077 0.02920769 0.02497829 0.02687991 0.0185481\n",
      " 0.02552714 0.0190592  0.02342979 0.01618253 0.02245636 0.01263591\n",
      " 0.01907991 0.01770202 0.0189357  0.02150764 0.02055182 0.0147183\n",
      " 0.01767698 0.01174149 0.01898586 0.01977923 0.01987778 0.01489787\n",
      " 0.01996713 0.01269635 0.01936363 0.00817069 0.01522693 0.0043996\n",
      " 0.0157113  0.00774593 0.01313899 0.00239492 0.01184735 0.00911001\n",
      " 0.01409832 0.01685076 0.0169857  0.01835989 0.01981038 0.02817957\n",
      " 0.01972416 0.02295979 0.01939655 0.02769264 0.02108634 0.02897385\n",
      " 0.01946239 0.03275357 0.02050479 0.03039664 0.02282473 0.03187296\n",
      " 0.0221601  0.02851282 0.02103932 0.03224938 0.02320563 0.03041736\n",
      " 0.02126034 0.02844721 0.02084965 0.03059866 0.02410539 0.0294176\n",
      " 0.02148763 0.03316797 0.02505688 0.03499136 0.02469165 0.03673704\n",
      " 0.02671846 0.03885051 0.02378248 0.03368943 0.02137633 0.03175382\n",
      " 0.01871311 0.02717464 0.01948903 0.02885298 0.01859554 0.02882362\n",
      " 0.01788859 0.0342178  0.02132147 0.03561124 0.02141395 0.03328194\n",
      " 0.02060354 0.03238751 0.02206134 0.03670423 0.02348152 0.04156314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data shape is:  (576,)\n",
      "1/1 [==============================] - 0s 11ms/sample\n",
      "prediction scaled is: [[0.07301249]]\n",
      "prediction is: [[18708.42]]\n",
      "1 record inserted.\n",
      "index_data shape is \n",
      "(4598, 3)\n",
      "trainingData groupData shape expecting 752 x 2304 (4281, 576)\n",
      "training label row 1 0.7322344985527829\n",
      "training label shape expecting 753 * 1  (4281,)\n",
      "input layer set to: 576\n",
      "4281/4281 [==============================] - 0s 64us/sample - loss: 0.0077 - mae: 0.0588\n",
      "4281/4281 [==============================] - 0s 38us/sample - loss: 0.0093 - mae: 0.0786\n",
      "mae: 7.86%\n",
      "prediction is: \n",
      "trainingData groupData shape expecting unsure (4598, 2)\n",
      "min in trainingData is:  14050.6\n",
      "max in trainingData is:  77845.45\n",
      "trainingData groupData shape expecting unsure (4598, 2)\n",
      "input data is:  [0.03168438 0.07275405 0.0314226  0.06827329 0.0311812  0.06829747\n",
      " 0.03135833 0.07423382 0.03068743 0.07499184 0.0295933  0.07879746\n",
      " 0.03418771 0.08268942 0.0316734  0.08203846 0.03297288 0.08271705\n",
      " 0.03426766 0.08315217 0.03597156 0.08267043 0.03491034 0.08775553\n",
      " 0.03840279 0.08833742 0.03901412 0.08858779 0.03729455 0.08773826\n",
      " 0.03847176 0.09075824 0.04011923 0.08872765 0.03881818 0.08736702\n",
      " 0.03878369 0.09060629 0.04011923 0.09232953 0.04072586 0.090406\n",
      " 0.04052208 0.08535543 0.0386771  0.08822519 0.03931195 0.08804043\n",
      " 0.03864262 0.08571976 0.0383009  0.08934236 0.04011766 0.09565168\n",
      " 0.04050327 0.09487121 0.04198458 0.09273357 0.04112244 0.08826663\n",
      " 0.04103936 0.0874948  0.03812533 0.08761394 0.03737763 0.08331966\n",
      " 0.03571291 0.08422272 0.03561886 0.08697334 0.03802345 0.08906091\n",
      " 0.03830403 0.0906909  0.03985118 0.08838577 0.0425003  0.08959791\n",
      " 0.04371356 0.0895392  0.0458595  0.09266278 0.04659937 0.09361937\n",
      " 0.0462263  0.09046816 0.04804306 0.09211369 0.04601625 0.09469855\n",
      " 0.04726871 0.0964615  0.04814182 0.09460703 0.04627646 0.09366253\n",
      " 0.04545978 0.09231917 0.04519801 0.09266796 0.04586107 0.09001404\n",
      " 0.04505693 0.08826145 0.04497385 0.09015735 0.04649435 0.08564379\n",
      " 0.04395339 0.08840304 0.04347843 0.08565069 0.04616047 0.08363565\n",
      " 0.04620592 0.08579919 0.04688623 0.08676786 0.04779226 0.08704241\n",
      " 0.04861991 0.07911518 0.04710725 0.08230264 0.04949616 0.08638799\n",
      " 0.04803836 0.08443856 0.04970307 0.07767512 0.04856818 0.0766063\n",
      " 0.04646457 0.0750471  0.04568864 0.07428563 0.04852743 0.07820176\n",
      " 0.05064986 0.08204709 0.05101039 0.08096619 0.05051035 0.0837427\n",
      " 0.05147594 0.08326959 0.05095709 0.08028069 0.05057932 0.07520422\n",
      " 0.04802739 0.07879401 0.04777188 0.07464478 0.04681569 0.07681523\n",
      " 0.04690818 0.0815947  0.0478722  0.08160506 0.04621376 0.08183989\n",
      " 0.0463423  0.08107669 0.04661191 0.08719435 0.04752735 0.0925868\n",
      " 0.04847413 0.09070817 0.04991469 0.09482287 0.05356075 0.09355548\n",
      " 0.05268764 0.09520965 0.05203241 0.09617659 0.05089282 0.09552736\n",
      " 0.04993977 0.09279573 0.05009182 0.09254364 0.04663229 0.09574837\n",
      " 0.05058089 0.09560506 0.05118438 0.09676367 0.05174085 0.09227255\n",
      " 0.0493253  0.09270422 0.04942562 0.09151108 0.04873277 0.08739983\n",
      " 0.0456181  0.08905745 0.04779853 0.09054931 0.04807441 0.09123653\n",
      " 0.04592377 0.08863959 0.0451886  0.08591315 0.0448265  0.08614798\n",
      " 0.04620122 0.08527082 0.04619025 0.0859615  0.04418068 0.08414502\n",
      " 0.04528265 0.08725306 0.04450202 0.08071927 0.04349724 0.08250985\n",
      " 0.04456002 0.080443   0.04602252 0.08675578 0.04718719 0.08621014\n",
      " 0.04766999 0.08582336 0.04777188 0.08599431 0.04826252 0.08175528\n",
      " 0.04749443 0.08007521 0.04510866 0.07816722 0.04556167 0.07408188\n",
      " 0.04349411 0.07143486 0.04327309 0.07424073 0.04408977 0.07659766\n",
      " 0.04424025 0.07616772 0.04453338 0.07345336 0.04295331 0.06837862\n",
      " 0.04134817 0.06760506 0.04251754 0.07726762 0.04241095 0.07344473\n",
      " 0.04180902 0.06903822 0.03957373 0.07386086 0.0405017  0.07942771\n",
      " 0.0453673  0.07799283 0.04688153 0.07975405 0.04847413 0.08186752\n",
      " 0.04695834 0.07826564 0.04569334 0.075332   0.0467091  0.07685149\n",
      " 0.04543    0.07956239 0.04649905 0.07631621 0.04641754 0.07404907\n",
      " 0.04605074 0.07239835 0.04651629 0.07169386 0.04550524 0.07230166\n",
      " 0.0459128  0.06582485 0.0436681  0.0658283  0.04391891 0.0681766\n",
      " 0.04369318 0.06310359 0.04404744 0.06270817 0.04291569 0.05929969\n",
      " 0.04294077 0.06097285 0.04180745 0.05835692 0.04027284 0.0577992\n",
      " 0.04070862 0.06488898 0.04168362 0.06513245 0.03915363 0.06094177\n",
      " 0.03913169 0.05758681 0.03696223 0.05338405 0.03593394 0.05426294\n",
      " 0.03530222 0.05020004 0.03150881 0.04968376 0.0341595  0.05388479\n",
      " 0.03440246 0.05368277 0.03534611 0.04849234 0.03409836 0.04349876\n",
      " 0.03202453 0.04495608 0.03431468 0.05355499 0.0358054  0.05066106\n",
      " 0.03776167 0.04475406 0.03521287 0.03682338 0.03254024 0.03852934\n",
      " 0.03289607 0.03706166 0.03213112 0.03621213 0.0297861  0.03207153\n",
      " 0.02698023 0.03331474 0.02936287 0.02731104 0.02595194 0.0190143\n",
      " 0.02438598 0.0121628  0.02432485 0.00672545 0.01868333 0.01795239\n",
      " 0.02068192 0.01679033 0.01648722 0.02026615 0.02010507 0.03160877\n",
      " 0.02395491 0.03256191 0.02415085 0.03475825 0.02344703 0.02817612\n",
      " 0.02159265 0.02316354 0.01805005 0.01589072 0.01622388 0.02202392\n",
      " 0.02139985 0.02628712 0.02347368 0.03333892 0.02582497 0.03417118\n",
      " 0.02576384 0.0329884  0.02629209 0.02820029 0.02528104 0.03695115\n",
      " 0.02780475 0.03958953 0.02822955 0.0392511  0.02947573 0.04465046\n",
      " 0.0317361  0.04138183 0.03085516 0.04433793 0.03335379 0.04762382\n",
      " 0.03247127 0.04226072 0.03230825 0.04400295 0.03102131 0.04077749\n",
      " 0.02923433 0.03664725 0.02876721 0.03664725 0.02797091 0.03622939\n",
      " 0.03023128 0.02687419 0.02633598 0.03073334 0.0279568  0.02723507\n",
      " 0.02612593 0.03088011 0.02867473 0.03310927 0.0279474  0.03473926\n",
      " 0.03097429 0.03474271 0.03053695 0.02924666 0.02962778 0.02996324\n",
      " 0.02963249 0.03028786 0.03211545 0.02596077 0.02920769 0.02497829\n",
      " 0.02687991 0.0185481  0.02552714 0.0190592  0.02342979 0.01618253\n",
      " 0.02245636 0.01263591 0.01907991 0.01770202 0.0189357  0.02150764\n",
      " 0.02055182 0.0147183  0.01767698 0.01174149 0.01898586 0.01977923\n",
      " 0.01987778 0.01489787 0.01996713 0.01269635 0.01936363 0.00817069\n",
      " 0.01522693 0.0043996  0.0157113  0.00774593 0.01313899 0.00239492\n",
      " 0.01184735 0.00911001 0.01409832 0.01685076 0.0169857  0.01835989\n",
      " 0.01981038 0.02817957 0.01972416 0.02295979 0.01939655 0.02769264\n",
      " 0.02108634 0.02897385 0.01946239 0.03275357 0.02050479 0.03039664\n",
      " 0.02282473 0.03187296 0.0221601  0.02851282 0.02103932 0.03224938\n",
      " 0.02320563 0.03041736 0.02126034 0.02844721 0.02084965 0.03059866\n",
      " 0.02410539 0.0294176  0.02148763 0.03316797 0.02505688 0.03499136\n",
      " 0.02469165 0.03673704 0.02671846 0.03885051 0.02378248 0.03368943\n",
      " 0.02137633 0.03175382 0.01871311 0.02717464 0.01948903 0.02885298\n",
      " 0.01859554 0.02882362 0.01788859 0.0342178  0.02132147 0.03561124\n",
      " 0.02141395 0.03328194 0.02060354 0.03238751 0.02206134 0.03670423\n",
      " 0.02348152 0.04156314 0.02459916 0.04077231 0.02523088 0.04135075\n",
      " 0.02314764 0.03656265 0.02232155 0.04289614 0.02685953 0.04227799\n",
      " 0.02867786 0.04183768 0.02777027 0.03843783 0.02410696 0.03772643\n",
      " 0.02517915 0.03501035 0.02411793 0.03642451 0.02382481 0.03142747\n",
      " 0.02118353 0.03451134 0.01948276 0.03468401 0.02094683 0.03386383\n",
      " 0.0196113  0.03087148 0.01788075 0.036055   0.02085748 0.03422643\n",
      " 0.02084024 0.03130833 0.02104715 0.02965589 0.02284981 0.03249975\n",
      " 0.0249738  0.03290379 0.02492207 0.03169165 0.02446592 0.02756141]\n",
      "input data shape is:  (576,)\n",
      "1/1 [==============================] - 0s 16ms/sample\n",
      "prediction scaled is: [[0.05129403]]\n",
      "prediction is: [[17322.895]]\n",
      "1 record inserted.\n",
      "index_data shape is \n",
      "(4598, 3)\n",
      "trainingData groupData shape expecting 752 x 2304 (4221, 576)\n",
      "training label row 1 0.7239106291495319\n",
      "training label shape expecting 753 * 1  (4221,)\n",
      "input layer set to: 576\n",
      "4221/4221 [==============================] - 0s 74us/sample - loss: 0.0087 - mae: 0.0693\n",
      "4221/4221 [==============================] - 0s 33us/sample - loss: 0.0051 - mae: 0.0624\n",
      "mae: 6.24%\n",
      "prediction is: \n",
      "trainingData groupData shape expecting unsure (4598, 2)\n",
      "min in trainingData is:  14050.6\n",
      "max in trainingData is:  77845.45\n",
      "trainingData groupData shape expecting unsure (4598, 2)\n",
      "input data is:  [0.04861991 0.07911518 0.04710725 0.08230264 0.04949616 0.08638799\n",
      " 0.04803836 0.08443856 0.04970307 0.07767512 0.04856818 0.0766063\n",
      " 0.04646457 0.0750471  0.04568864 0.07428563 0.04852743 0.07820176\n",
      " 0.05064986 0.08204709 0.05101039 0.08096619 0.05051035 0.0837427\n",
      " 0.05147594 0.08326959 0.05095709 0.08028069 0.05057932 0.07520422\n",
      " 0.04802739 0.07879401 0.04777188 0.07464478 0.04681569 0.07681523\n",
      " 0.04690818 0.0815947  0.0478722  0.08160506 0.04621376 0.08183989\n",
      " 0.0463423  0.08107669 0.04661191 0.08719435 0.04752735 0.0925868\n",
      " 0.04847413 0.09070817 0.04991469 0.09482287 0.05356075 0.09355548\n",
      " 0.05268764 0.09520965 0.05203241 0.09617659 0.05089282 0.09552736\n",
      " 0.04993977 0.09279573 0.05009182 0.09254364 0.04663229 0.09574837\n",
      " 0.05058089 0.09560506 0.05118438 0.09676367 0.05174085 0.09227255\n",
      " 0.0493253  0.09270422 0.04942562 0.09151108 0.04873277 0.08739983\n",
      " 0.0456181  0.08905745 0.04779853 0.09054931 0.04807441 0.09123653\n",
      " 0.04592377 0.08863959 0.0451886  0.08591315 0.0448265  0.08614798\n",
      " 0.04620122 0.08527082 0.04619025 0.0859615  0.04418068 0.08414502\n",
      " 0.04528265 0.08725306 0.04450202 0.08071927 0.04349724 0.08250985\n",
      " 0.04456002 0.080443   0.04602252 0.08675578 0.04718719 0.08621014\n",
      " 0.04766999 0.08582336 0.04777188 0.08599431 0.04826252 0.08175528\n",
      " 0.04749443 0.08007521 0.04510866 0.07816722 0.04556167 0.07408188\n",
      " 0.04349411 0.07143486 0.04327309 0.07424073 0.04408977 0.07659766\n",
      " 0.04424025 0.07616772 0.04453338 0.07345336 0.04295331 0.06837862\n",
      " 0.04134817 0.06760506 0.04251754 0.07726762 0.04241095 0.07344473\n",
      " 0.04180902 0.06903822 0.03957373 0.07386086 0.0405017  0.07942771\n",
      " 0.0453673  0.07799283 0.04688153 0.07975405 0.04847413 0.08186752\n",
      " 0.04695834 0.07826564 0.04569334 0.075332   0.0467091  0.07685149\n",
      " 0.04543    0.07956239 0.04649905 0.07631621 0.04641754 0.07404907\n",
      " 0.04605074 0.07239835 0.04651629 0.07169386 0.04550524 0.07230166\n",
      " 0.0459128  0.06582485 0.0436681  0.0658283  0.04391891 0.0681766\n",
      " 0.04369318 0.06310359 0.04404744 0.06270817 0.04291569 0.05929969\n",
      " 0.04294077 0.06097285 0.04180745 0.05835692 0.04027284 0.0577992\n",
      " 0.04070862 0.06488898 0.04168362 0.06513245 0.03915363 0.06094177\n",
      " 0.03913169 0.05758681 0.03696223 0.05338405 0.03593394 0.05426294\n",
      " 0.03530222 0.05020004 0.03150881 0.04968376 0.0341595  0.05388479\n",
      " 0.03440246 0.05368277 0.03534611 0.04849234 0.03409836 0.04349876\n",
      " 0.03202453 0.04495608 0.03431468 0.05355499 0.0358054  0.05066106\n",
      " 0.03776167 0.04475406 0.03521287 0.03682338 0.03254024 0.03852934\n",
      " 0.03289607 0.03706166 0.03213112 0.03621213 0.0297861  0.03207153\n",
      " 0.02698023 0.03331474 0.02936287 0.02731104 0.02595194 0.0190143\n",
      " 0.02438598 0.0121628  0.02432485 0.00672545 0.01868333 0.01795239\n",
      " 0.02068192 0.01679033 0.01648722 0.02026615 0.02010507 0.03160877\n",
      " 0.02395491 0.03256191 0.02415085 0.03475825 0.02344703 0.02817612\n",
      " 0.02159265 0.02316354 0.01805005 0.01589072 0.01622388 0.02202392\n",
      " 0.02139985 0.02628712 0.02347368 0.03333892 0.02582497 0.03417118\n",
      " 0.02576384 0.0329884  0.02629209 0.02820029 0.02528104 0.03695115\n",
      " 0.02780475 0.03958953 0.02822955 0.0392511  0.02947573 0.04465046\n",
      " 0.0317361  0.04138183 0.03085516 0.04433793 0.03335379 0.04762382\n",
      " 0.03247127 0.04226072 0.03230825 0.04400295 0.03102131 0.04077749\n",
      " 0.02923433 0.03664725 0.02876721 0.03664725 0.02797091 0.03622939\n",
      " 0.03023128 0.02687419 0.02633598 0.03073334 0.0279568  0.02723507\n",
      " 0.02612593 0.03088011 0.02867473 0.03310927 0.0279474  0.03473926\n",
      " 0.03097429 0.03474271 0.03053695 0.02924666 0.02962778 0.02996324\n",
      " 0.02963249 0.03028786 0.03211545 0.02596077 0.02920769 0.02497829\n",
      " 0.02687991 0.0185481  0.02552714 0.0190592  0.02342979 0.01618253\n",
      " 0.02245636 0.01263591 0.01907991 0.01770202 0.0189357  0.02150764\n",
      " 0.02055182 0.0147183  0.01767698 0.01174149 0.01898586 0.01977923\n",
      " 0.01987778 0.01489787 0.01996713 0.01269635 0.01936363 0.00817069\n",
      " 0.01522693 0.0043996  0.0157113  0.00774593 0.01313899 0.00239492\n",
      " 0.01184735 0.00911001 0.01409832 0.01685076 0.0169857  0.01835989\n",
      " 0.01981038 0.02817957 0.01972416 0.02295979 0.01939655 0.02769264\n",
      " 0.02108634 0.02897385 0.01946239 0.03275357 0.02050479 0.03039664\n",
      " 0.02282473 0.03187296 0.0221601  0.02851282 0.02103932 0.03224938\n",
      " 0.02320563 0.03041736 0.02126034 0.02844721 0.02084965 0.03059866\n",
      " 0.02410539 0.0294176  0.02148763 0.03316797 0.02505688 0.03499136\n",
      " 0.02469165 0.03673704 0.02671846 0.03885051 0.02378248 0.03368943\n",
      " 0.02137633 0.03175382 0.01871311 0.02717464 0.01948903 0.02885298\n",
      " 0.01859554 0.02882362 0.01788859 0.0342178  0.02132147 0.03561124\n",
      " 0.02141395 0.03328194 0.02060354 0.03238751 0.02206134 0.03670423\n",
      " 0.02348152 0.04156314 0.02459916 0.04077231 0.02523088 0.04135075\n",
      " 0.02314764 0.03656265 0.02232155 0.04289614 0.02685953 0.04227799\n",
      " 0.02867786 0.04183768 0.02777027 0.03843783 0.02410696 0.03772643\n",
      " 0.02517915 0.03501035 0.02411793 0.03642451 0.02382481 0.03142747\n",
      " 0.02118353 0.03451134 0.01948276 0.03468401 0.02094683 0.03386383\n",
      " 0.0196113  0.03087148 0.01788075 0.036055   0.02085748 0.03422643\n",
      " 0.02084024 0.03130833 0.02104715 0.02965589 0.02284981 0.03249975\n",
      " 0.0249738  0.03290379 0.02492207 0.03169165 0.02446592 0.02756141\n",
      " 0.02167887 0.02855599 0.02284354 0.02868722 0.02449728 0.03593758\n",
      " 0.02675608 0.03583053 0.02836122 0.04088455 0.02828755 0.03939096\n",
      " 0.02831577 0.03624493 0.02752417 0.04062382 0.02741914 0.04062036\n",
      " 0.0278847  0.04031129 0.02840668 0.04164947 0.0285023  0.03835668\n",
      " 0.02682348 0.03746398 0.02645198 0.03429032 0.02643787 0.03078169\n",
      " 0.02436874 0.02851109 0.02421982 0.03073507 0.02439695 0.02431178\n",
      " 0.01939812 0.02360384 0.01837609 0.02508361 0.01938558 0.02023507\n",
      " 0.01879619 0.02298223 0.01763935 0.02413048 0.01800929 0.02112777\n",
      " 0.01576303 0.0200296  0.01313429 0.01874149 0.01474727 0.01664874\n",
      " 0.0135873  0.01821139 0.01279414 0.01653996 0.01324402 0.01400518\n",
      " 0.00926564 0.01370819 0.00779373 0.01805599 0.01043658 0.02211371\n",
      " 0.01205897 0.02065466 0.00836901 0.01867242 0.00845993 0.02142131\n",
      " 0.0114445  0.01755525 0.0068297  0.01904193 0.00913083 0.01639319\n",
      " 0.00667138 0.018833   0.00705543 0.01979995 0.00882673 0.01822693\n",
      " 0.00860101 0.01504464 0.00582806 0.01706487 0.00588762 0.01515688\n",
      " 0.0033404  0.01684903 0.00312878 0.01153601 0.00171957 0.00986112\n",
      " 0.00138883 0.01080389 0.         0.01770202 0.00429972 0.01804563\n",
      " 0.00366487 0.02539097 0.0115166  0.02630611 0.01247123 0.02820202\n",
      " 0.01405286 0.02861642 0.01410772 0.03361864 0.01348541 0.02577429\n",
      " 0.01328791 0.02838677 0.01641982 0.02720399 0.01653582 0.02687937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data shape is:  (576,)\n",
      "1/1 [==============================] - 0s 12ms/sample\n",
      "prediction scaled is: [[0.0678926]]\n",
      "prediction is: [[18381.797]]\n",
      "1 record inserted.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  passwd=\"\",\n",
    "  database=\"a3d\"\n",
    ")\n",
    "\n",
    "def db_upload(sql, val):\n",
    "  mycursor = mydb.cursor()\n",
    "  mycursor.execute(sql, val)\n",
    "  mydb.commit()\n",
    "  print(mycursor.rowcount, \"record inserted.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#function to create trainingArray that groups data into lists of 'groupSize' date rows (atm 288) * all indices' data (8)\n",
    "#creating a master list of 780 rows of 2304 columns, essentially 780 moving windows of all data\n",
    "def groupData( data, groupSize ):\n",
    "\ttrainingArray = []\n",
    "\tfor i in range( data.size ):\n",
    "\t\tif i >= groupSize and i <= (data.shape[0]-days_indicator) :\n",
    "\t\t\ttrainingListSingleEntry = []\n",
    "\t\t\tfor ii in range( groupSize ):\n",
    "\t\t\t\ttrainingListSingleEntry = np.concatenate( ( trainingListSingleEntry, data[ i - ii -1] ), axis=None )\n",
    "\t\t\ttrainingArray.append( trainingListSingleEntry )\n",
    "\treturn trainingArray\n",
    "\n",
    "\n",
    "def groupLabels( data, groupSize ):\n",
    "\ttrainingArray = []\n",
    "\tfor i in range( data.size):\n",
    "\t\tif i >= (groupSize+days_indicator-1):\n",
    "\t\t\ttrainingListSingleEntry = data[i]\n",
    "\t\t\ttrainingArray.append( trainingListSingleEntry )\n",
    "\treturn trainingArray\n",
    "\n",
    "def doTraining():\n",
    "\ttrainingData = index_data[ subset ].values\n",
    "\ttrainingData = minmax_scale( trainingData )\n",
    "\ttrainingData = groupData( trainingData, groupSize = groupSize )\n",
    "\ttrainingData = np.array( trainingData )\n",
    "# \tprint(\"training data 1\", trainingData[0])\n",
    "\tprint(\"trainingData groupData shape expecting 752 x 2304\", trainingData.shape)\n",
    "\n",
    "#trainingLabels being target, output dataset\n",
    "\ttrainingLabels = index_data[ 'AXJOA' ].values\n",
    "\ttrainingLabels = minmax_scale( trainingLabels )\n",
    "# \tprint(\"training label size is \")\n",
    "# \tprint(trainingLabels.size)\n",
    "\ttrainingLabels = groupLabels( trainingLabels, groupSize = groupSize )\n",
    "\ttrainingLabels = np.array( trainingLabels )\n",
    "\tprint(\"training label row 1\",trainingLabels[0])\n",
    "\tprint(\"training label shape expecting 753 * 1 \", trainingLabels.shape)\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\tprint(\"input layer set to:\", len( trainingData[0] ))\n",
    "\tmodel.add( Dense( 32, input_dim = len( trainingData[0] ), kernel_initializer = 'normal', activation = 'relu' ) ) # going into a 32-unit layer\n",
    "\tmodel.add( Dense( 32, kernel_initializer = 'normal', activation = 'relu' ) ) # Another hidden layer of 16 units\n",
    "\tmodel.add( Dense( 32, kernel_initializer = 'normal', activation = 'relu' ) ) # Another hidden layer of 16 units\n",
    "\tmodel.add( Dense( 1 ) ) # Output layer with a binary classification ( Democrat or Republican )\n",
    "\tmodel.compile( loss = 'mse', optimizer = 'rmsprop', metrics = [ 'mae' ] ) # Compile model\n",
    "\tmodel.fit( trainingData, trainingLabels, epochs = 1, batch_size = 50, verbose = verbosity ) # Train model\n",
    "\tscores = model.evaluate( trainingData, trainingLabels, verbose = verbosity ) # Grade the model\n",
    "\tprint( \"%s: %.2f%%\" % ( model.metrics_names[1], scores[1]*100 ) )\n",
    "\tmodel.save( 'BHMarket_Model.h5' ) # Save the model\n",
    "# \tplot_model(model, to_file='modelplotbh.png')\n",
    "    \n",
    "def doPrediction():\n",
    "\ttrainingData = index_data[ subset ].values\n",
    "\tprint(\"trainingData groupData shape expecting unsure\", trainingData.shape)\n",
    "\tASAmin =  float((trainingData.min(axis=0))[0])\n",
    "\tprint(\"min in trainingData is: \", ASAmin)\n",
    "\tASAmax =  float((trainingData.max(axis=0))[0])\n",
    "\tprint(\"max in trainingData is: \", ASAmax)\n",
    "\tASArange =  ASAmax-ASAmin\n",
    "\tprint(\"trainingData groupData shape expecting unsure\", trainingData.shape)\n",
    "\ttrainingData = minmax_scale( trainingData )\n",
    "\ttrainingData = groupData( trainingData, groupSize = groupSize )\n",
    "\tinputData = trainingData[-1]\n",
    "\tprint( \"input data is: \", inputData )\n",
    "\n",
    "\ttrainingLabels = index_data[ 'Date' ].values\n",
    "\ttrainingLabels = groupLabels( trainingLabels, groupSize = groupSize )\n",
    "# \tdate = trainingLabels[-1]\n",
    "# \tprint( \"training data shape is\", inputData.shape )\n",
    "\n",
    "\tloaded_model = load_model( 'BHMarket_Model.h5' )\n",
    "\n",
    "\tsql = \"INSERT INTO daily_ai_indicators (metric, value) VALUES (%s, %s)\"\n",
    "\t# loaded_model = load_model( 'LSMarket_Model.h5' )\n",
    "\t# evaluate loaded model on test data\n",
    "\tloaded_model.compile( loss = 'mse', optimizer = 'rmsprop', metrics = [ 'mae' ] )\n",
    "\t# Predict things...\n",
    "\tprint( \"input data shape is: \", inputData.shape )\n",
    "\tprediction_1 = loaded_model.predict( inputData.reshape( (1, 576) ), batch_size = None, verbose = verbosity, steps = None )\n",
    "\tprint(\"prediction scaled is:\", prediction_1)    \n",
    "\tprediction_1=prediction_1*ASArange+ASAmin\n",
    "\tprint(\"prediction is:\", prediction_1)\n",
    "\tx123=prediction_1.tolist()\n",
    "\tfinal_int=(prediction_1.tolist())[0][0]\n",
    "\tmetric='t_'+str(forecast_array[i])\n",
    "\tval = (metric, final_int)\n",
    "\tdb_upload(sql,val)\n",
    "    \n",
    "#     print('t_'+str(i))\n",
    "#     i=i+1 \n",
    "# forecast_array=(1,7,30,90) # being t+1, 7, 30, 90 days array\n",
    "\n",
    "forecast_array=(1,7,30,90)\n",
    "array_length=len(forecast_array)\n",
    "i=0\n",
    "\n",
    "while (i<array_length):\n",
    "\n",
    "    verbosity = 1\n",
    "    groupSize = 288\n",
    "    days_indicator=forecast_array[i]  #set equal to t+X days indicator\n",
    "    index_names = [ 'Date', 'AXJOA', 'SPXTR']\n",
    "    subset = [ 'AXJOA', 'SPXTR' ]\n",
    "    index_data = pd.read_csv( 'spxt_axjoa.csv', names = index_names ) # read file, insert header row\n",
    "    index_data.dropna( inplace = True ) \n",
    "    print(\"index_data shape is \")\n",
    "    print(index_data.shape)\n",
    "    index_data.to_csv('out.csv', index=False)\n",
    "    ASAMin=0\n",
    "    ASAMax=0\n",
    "    ASArange=0\n",
    "    doTraining()\n",
    "    print(\"prediction is: \")\n",
    "    doPrediction()\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  passwd=\"\",\n",
    "  database=\"a3d\"\n",
    ")\n",
    "\n",
    "def db_upload(sql, val):\n",
    "  mycursor = mydb.cursor()\n",
    "  mycursor.execute(sql, val)\n",
    "  mydb.commit()\n",
    "  print(mycursor.rowcount, \"record inserted.\")\n",
    "\n",
    "\n",
    "sql = \"INSERT INTO daily_ai_indicators (metric, value) VALUES (%s, %s)\"\n",
    "val = (\"delta_strategy\", 123477)\n",
    "db_upload(sql,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0\n",
      "t_1\n",
      "t_2\n",
      "t_3\n"
     ]
    }
   ],
   "source": [
    "forecast_array=(1,7,30,90)\n",
    "\n",
    "\n",
    "array_length=(len(forecast_array))\n",
    "i=0\n",
    "while (i<array_length):\n",
    "    print('t_'+str(i))\n",
    "    i=i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
